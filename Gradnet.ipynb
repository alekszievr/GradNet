{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    from chainer.cuda import cupy as cp\n",
    "except:\n",
    "    pass\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from copy import deepcopy\n",
    "from chainer.datasets import get_cifar10\n",
    "from linalg import maxpercentile\n",
    "import time\n",
    "import net\n",
    "import augmentation\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities:\n",
    "def conv_distortion(databatch, device, test=False):\n",
    "   \"\"\"converter with augmentation\"\"\"\n",
    "   batchsize = len(databatch)\n",
    "   inputdata = [datatuple[0] for datatuple in databatch]\n",
    "   labels = np.array([datatuple[1] for datatuple in databatch])\n",
    "   if device>=0:\n",
    "      inputdata = chainer.cuda.to_gpu(inputdata, device)\n",
    "      labels = chainer.cuda.to_gpu(labels, device)\n",
    "   distorted_input = augmentation.distortion_batch(inputdata, device, test=test)\n",
    "   return (distorted_input, labels) \n",
    "\n",
    "def grads (optimizer, data, label, mins=None, ranges=None, xp=np, percentage=15, exponent=0.5):\n",
    "   opt = deepcopy (optimizer)\n",
    "   model = opt.target\n",
    "   net = opt.target.predictor\n",
    "   model.cleargrads ()\n",
    "   opt.update (model, data, label)\n",
    "   listoflinks = list(net.links(skipself=True))\n",
    "   gradients = xp.concatenate([maxpercentile(xp.ravel(link.W.grad), 15, xp) for link in listoflinks])\n",
    "   gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "   # scale norm + power norm:\n",
    "   normalized = xp.divide ((gradients-mins), xp.where (ranges!=0, ranges, 1))\n",
    "   n = xp.where (gradients!=0, normalized, 0)\n",
    "   out = xp.sign(n)*xp.power(xp.abs(n), exponent)\n",
    "   return out\n",
    "\n",
    "def grad_gen_new (updater, iterator, mins=None, ranges=None, test=False, xp=np, realbatchsize=25, percentage=15, exponent=0.125):\n",
    "   for (i, batch)  in enumerate (iterator):\n",
    "      batchsize=len (batch)\n",
    "      in_arr, truelabels = updater.converter (batch, updater.device)\n",
    "      random_labels = xp.random.randint (10, size=(batchsize,))\n",
    "      optimizer = updater._optimizers ['main']\n",
    "      gradients = xp.concatenate(([xp.expand_dims(grads(optimizer, xp.expand_dims(in_arr[j], 0), xp.expand_dims(random_labels[j], 0), mins, ranges, xp, percentage, exponent), axis=0) for j in range(batchsize)]))\n",
    "      gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "      for k in range (int (batchsize/realbatchsize)):\n",
    "         g = gradients [k*realbatchsize:(k+1)* realbatchsize, :]\n",
    "         o = truelabels [k*realbatchsize:(k+1)*realbatchsize]\n",
    "         yield g, o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset:\n",
    "train, test = get_cifar10()\n",
    "train_CNN = train [0:int (len (train)/2) ]\n",
    "train_gradnet = train [int (len (train)/2)  : len (train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_CNN=64\n",
    "learnrate_CNN=0.05\n",
    "epoch_CNN=100\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.66335     1.64143               0.375          0.407643                  219.548       \n",
      "\u001b[J2           1.48936     1.4274                0.390625       0.493133                  450.515       \n",
      "\u001b[J3           1.28255     1.38811               0.53125        0.506568                  687.25        \n",
      "\u001b[J4           1.24593     1.28992               0.5625         0.55006                   903.729       \n",
      "\u001b[J5           1.1699      1.28625               0.59375        0.543591                  1123.46       \n",
      "\u001b[J6           1.39983     1.26058               0.484375       0.562998                  1329.22       \n",
      "\u001b[J7           1.2491      1.23249               0.515625       0.570263                  1541.81       \n",
      "\u001b[J8           1.22174     1.16325               0.578125       0.596238                  1765.65       \n",
      "\u001b[J9           1.20136     1.11704               0.578125       0.619327                  1981.04       \n",
      "\u001b[J10          1.30015     1.12251               0.53125        0.615048                  2191.15       \n",
      "\u001b[J11          1.04001     1.10222               0.609375       0.619725                  2411.18       \n",
      "\u001b[J12          0.908207    1.08599               0.703125       0.62699                   2624.22       \n",
      "\u001b[J13          1.15758     1.08753               0.59375        0.621616                  2836.76       \n",
      "\u001b[J14          0.857806    1.06665               0.765625       0.628483                  3063.32       \n",
      "\u001b[J15          1.16753     1.05959               0.59375        0.643213                  3290.93       \n",
      "\u001b[J16          0.897269    1.05164               0.6875         0.643113                  3521.32       \n",
      "\u001b[J17          1.1555      0.985687              0.625          0.661525                  3723.32       \n",
      "\u001b[J18          0.814682    1.01484               0.78125        0.647691                  3930.83       \n",
      "\u001b[J19          0.911532    0.983511              0.671875       0.665605                  4141.34       \n",
      "\u001b[J20          0.966688    0.982785              0.6875         0.664212                  4348.14       \n",
      "\u001b[J21          0.783327    0.978098              0.734375       0.665804                  4543.24       \n",
      "\u001b[J22          0.793921    0.989326              0.734375       0.661425                  4737.24       \n",
      "\u001b[J23          1.1292      1.00316               0.625          0.661823                  4936.03       \n",
      "\u001b[J24          0.875197    0.983722              0.671875       0.661127                  5130.25       \n",
      "\u001b[J25          0.820531    0.934132              0.703125       0.681429                  5370.61       \n",
      "\u001b[J26          0.788345    0.911986              0.71875        0.6874                    5628          \n",
      "\u001b[J27          0.840607    0.889889              0.6875         0.699244                  5870.85       \n",
      "\u001b[J28          0.772275    0.911145              0.765625       0.686803                  6116.57       \n"
     ]
    }
   ],
   "source": [
    "# Training of the original CNN network:\n",
    "model_CNN = L.Classifier (net.cnn_cifar ())\n",
    "if gpu >= 0:\n",
    "    # Make a specified GPU current\n",
    "    chainer.cuda.get_device_from_id(gpu).use()\n",
    "    model_CNN.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "optimizer_CNN = chainer.optimizers.SGD(learnrate_CNN)\n",
    "optimizer_CNN.setup(model_CNN)\n",
    "optimizer_CNN.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "train_iter_CNN = chainer.iterators.SerialIterator(train_CNN, batchsize_CNN)\n",
    "test_iter_CNN = chainer.iterators.SerialIterator(test, batchsize_CNN,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_CNN = training.StandardUpdater(train_iter_CNN, optimizer_CNN, converter=conv_distortion, device=gpu)\n",
    "trainer_CNN = training.Trainer(updater_CNN, (epoch_CNN, 'epoch'), out=\"result_CNN\")\n",
    "\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer_CNN.extend(extensions.Evaluator(test_iter_CNN, model_CNN, converter=conv_distortion, device=gpu), trigger=(1, \"epoch\"))\n",
    "\n",
    "# Reduce the learning rate by half every 25 epochs.\n",
    "trainer_CNN.extend(extensions.ExponentialShift('lr', 0.5),\n",
    "               trigger=(25, 'epoch'))\n",
    "\n",
    "# Take a snapshot at each epoch\n",
    "trainer_CNN.extend(extensions.snapshot(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer_CNN.extend(extensions.LogReport(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer_CNN.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']), trigger = (1, 'epoch'))\n",
    "\n",
    "trainer_CNN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "500\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result_gradnet/snapshot_iter_500'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3ad3276bf160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result_gradnet/snapshot_iter_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/serializers/npz.py\u001b[0m in \u001b[0;36msave_npz\u001b[0;34m(file, obj, compression)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0msave_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result_gradnet/snapshot_iter_500'"
     ]
    }
   ],
   "source": [
    "batchsize=50\n",
    "learnrate=0.05\n",
    "out=\"result\"\n",
    "epoch=200\n",
    "gpu=0\n",
    "original=23047\n",
    "normalization=\"std\"\n",
    "percentage=15\n",
    "exponent=0.5\n",
    "sizes = [5, 100, 25]\n",
    "\n",
    "train, test = get_cifar10()\n",
    "\n",
    "model_old = L.Classifier(net.cnn_cifar())\n",
    "\n",
    "optimizer_old = chainer.optimizers.SGD(learnrate)\n",
    "optimizer_old.setup(model_old)\n",
    "optimizer_old.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "    \n",
    "train_iter_old = chainer.iterators.SerialIterator (train [0:int (len (train)/2) ], 1, repeat=False, shuffle=False)\n",
    "test_iter_old = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_old = training.StandardUpdater(train_iter_old, optimizer_old, converter=conv_distortion, device=gpu)\n",
    "trainer_old = training.Trainer(updater_old, (1000, 'epoch'), out=out)\n",
    "\n",
    "chainer.serializers.load_npz ('./result_CNN/snapshot_iter_{}'.format(original), trainer_old, strict=False)\n",
    "\n",
    "if not os.path.exists('./result_gradnet/'):\n",
    "    os.mkdir('./result_gradnet/')\n",
    "\n",
    "if gpu >= 0:\n",
    "   # Make a specified GPU current\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model_old.to_gpu(gpu)  # Copy the model to the GPU\n",
    "   xp = cp\n",
    "else:\n",
    "   xp = np\n",
    "\n",
    "mins=xp.load(\"CNNgradmin_{}.npy\".format(original))\n",
    "maxes=xp.load(\"CNNgradmax_{}.npy\".format(original))\n",
    "ranges = maxes-mins\n",
    "\n",
    "linksizes = [link.W.size for link in list(model_old.predictor.links(skipself=True))]\n",
    "dividers = xp.cumsum(xp.array(linksizes))[:-1].tolist()\n",
    "\n",
    "model=net.gradnet(input_dividers = dividers, middle_sizes = sizes)\n",
    "\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "if gpu >=0 :\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model.to_gpu(gpu)\n",
    "\n",
    "train_iter_new = chainer.iterators.SerialIterator (train [int (len (train)/2)  : len (train)], 25, repeat=True, shuffle=True)\n",
    "gg = grad_gen_new (updater_old, train_iter_new, mins, ranges, test=False, xp=xp, realbatchsize=25, percentage=percentage, exponent=exponent)\n",
    "\n",
    "t0=time.time()\n",
    "while train_iter_new.epoch < epoch:\n",
    "   grads_train, target_train = gg.__next__()\n",
    "   grads_train = chainer.Variable(grads_train)\n",
    "   target_train = chainer.Variable(target_train)\n",
    "   if gpu >=0:\n",
    "      grads_train.to_gpu(gpu)\n",
    "      target_train.to_gpu(gpu)\n",
    "\n",
    "   # Calculate the prediction of the network\n",
    "   prediction_train = model(grads_train)\n",
    "   # Calculate the loss with softmax_cross_entropy\n",
    "   loss = chainer.functions.softmax_cross_entropy(prediction_train, target_train)\n",
    "   # Calculate the gradients in the network\n",
    "   model.cleargrads()\n",
    "   loss.backward()\n",
    "   # Update all the trainable paremters\n",
    "   optimizer.update()\n",
    "   if optimizer.t%50==0:\n",
    "      print(optimizer.t)\n",
    "                      \n",
    "   if optimizer.t%500==0:\n",
    "      print (optimizer.t)\n",
    "      chainer.serializers.save_npz('result_gradnet/snapshot_iter_{}'.format(optimizer.t), model)\n",
    "      t1=time.time()\n",
    "      print(\"time:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
