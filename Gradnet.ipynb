{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of GradNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    from chainer.cuda import cupy as cp\n",
    "except:\n",
    "    pass\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from copy import deepcopy\n",
    "from chainer.datasets import get_cifar10\n",
    "from linalg import maxpercentile\n",
    "import time\n",
    "import net\n",
    "import augmentation\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_distortion(databatch, device, test=False):\n",
    "   \"\"\"Applies augmentation to the images and converts databatch into a tuple of form (images, labels) where both 'images' and 'labels' are arrays.\n",
    "      Moves data to gpu if device>=0.\"\"\"\n",
    "   if device>=0:\n",
    "        xp=cp\n",
    "   else:\n",
    "        xp=np\n",
    "   batchsize = len(databatch)\n",
    "   inputdata = [datatuple[0] for datatuple in databatch]\n",
    "   labels = xp.array([datatuple[1] for datatuple in databatch])\n",
    "   if device>=0:\n",
    "      inputdata = chainer.cuda.to_gpu(inputdata, device)\n",
    "      labels = chainer.cuda.to_gpu(labels, device)\n",
    "   distorted_input = augmentation.distortion_batch(inputdata, device, test=test)\n",
    "   return (distorted_input, labels) \n",
    "\n",
    "def grads (optimizer, data, label, mins=None, ranges=None, xp=np, percentage=15, exponent=0.5):\n",
    "   \"\"\"Creates gradient array for a given (data, label) pair. The neural net is contained by 'optimizer'.\n",
    "      'mins' and 'ranges' are arrays needed for scaling the gradients, 'exponent' is the exponent of the power norm.\n",
    "      'percentage' determines where we cut the gradient values to throw away the small ones.\"\"\"\n",
    "   opt = deepcopy (optimizer)\n",
    "   model = opt.target\n",
    "   net = opt.target.predictor\n",
    "   model.cleargrads ()\n",
    "   opt.update (model, data, label)\n",
    "   listoflinks = list(net.links(skipself=True))\n",
    "   gradients = xp.concatenate([maxpercentile(xp.ravel(link.W.grad), 15, xp) for link in listoflinks])\n",
    "   gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "   # scale norm + power norm:\n",
    "   normalized = xp.divide ((gradients-mins), xp.where (ranges!=0, ranges, 1))\n",
    "   n = xp.where (gradients!=0, normalized, 0)\n",
    "   out = xp.sign(n)*xp.power(xp.abs(n), exponent)\n",
    "   return out\n",
    "\n",
    "def grad_gen_new (updater, iterator, mins=None, ranges=None, test=False, xp=np, realbatchsize=25, percentage=15, exponent=0.5):\n",
    "   \"\"\"Generator that returns normalized gradient arrays given a dataset iterator and an updater. \n",
    "      The iterator gives data batches, the updater contains the neural net.\"\"\"\n",
    "   for (i, batch)  in enumerate (iterator):\n",
    "      batchsize=len (batch)\n",
    "      in_arr, truelabels = updater.converter (batch, updater.device)\n",
    "      random_labels = xp.random.randint (10, size=(batchsize,))\n",
    "      optimizer = updater._optimizers ['main']\n",
    "      gradients = xp.concatenate(([xp.expand_dims(grads(optimizer, xp.expand_dims(in_arr[j], 0), xp.expand_dims(random_labels[j], 0), mins, ranges, xp, percentage, exponent), axis=0) for j in range(batchsize)]))\n",
    "      gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "      for k in range (int (batchsize/realbatchsize)):\n",
    "         g = gradients [k*realbatchsize:(k+1)* realbatchsize, :]\n",
    "         o = truelabels [k*realbatchsize:(k+1)*realbatchsize]\n",
    "         yield g, o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Cifar-10 dataset. \n",
    "\n",
    "We split the training data into two equally sized sets. We use the first one to train the original CNN network and the second one to train the GradNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset:\n",
    "train, test = get_cifar10()\n",
    "train_CNN = train [0:int (len (train)/2) ]\n",
    "train_gradnet = train [int (len (train)/2)  : len (train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our pre-trained GradNet using Cifar-10 or other data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the original CNN network.\n",
    "\n",
    "This can take a while. A pre-trained model can be found in the repository. To skip this training, jump to the \"training the GradNet\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_CNN=64\n",
    "learnrate_CNN=0.05\n",
    "epoch_CNN=100\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.48478     1.63433               0.421875       0.403165                  232.612       \n",
      "\u001b[J2           1.27392     1.48166               0.609375       0.46865                   462.268       \n",
      "\u001b[J3           1.25437     1.38152               0.546875       0.503583                  679.968       \n",
      "\u001b[J4           1.42943     1.29438               0.515625       0.542596                  903.339       \n",
      "\u001b[J5           1.19399     1.26791               0.59375        0.554439                  1124.06       \n",
      "\u001b[J6           1.24174     1.24069               0.59375        0.562898                  1357.5        \n",
      "\u001b[J7           1.14403     1.22494               0.53125        0.583002                  1574.54       \n",
      "\u001b[J8           1.00215     1.1674                0.671875       0.592357                  1812.38       \n",
      "\u001b[J9           1.02669     1.18634               0.625          0.584096                  2039.92       \n",
      "\u001b[J10          0.994309    1.13364               0.65625        0.600916                  2276.79       \n",
      "\u001b[J11          1.08708     1.13362               0.640625       0.607285                  2507.51       \n",
      "\u001b[J12          0.952616    1.16692               0.65625        0.605494                  2733.88       \n",
      "\u001b[J13          1.16696     1.12665               0.578125       0.618531                  2952.08       \n",
      "\u001b[J14          0.913735    1.05172               0.6875         0.640028                  3170.92       \n",
      "\u001b[J15          1.12582     1.04442               0.5625         0.634952                  3380.72       \n",
      "\u001b[J16          1.01378     1.01031               0.6875         0.650975                  3595.79       \n",
      "\u001b[J17          0.778743    1.01667               0.796875       0.649283                  3808.05       \n",
      "\u001b[J18          0.808866    1.048                 0.734375       0.632365                  4028.15       \n",
      "\u001b[J19          0.94933     0.982778              0.609375       0.658141                  4243.48       \n"
     ]
    }
   ],
   "source": [
    "# Training of the original CNN network:\n",
    "model_CNN = L.Classifier (net.cnn_cifar ())\n",
    "if gpu >= 0:\n",
    "    # Make a specified GPU current\n",
    "    chainer.cuda.get_device_from_id(gpu).use()\n",
    "    model_CNN.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "optimizer_CNN = chainer.optimizers.SGD(learnrate_CNN)\n",
    "optimizer_CNN.setup(model_CNN)\n",
    "optimizer_CNN.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "train_iter_CNN = chainer.iterators.SerialIterator(train_CNN, batchsize_CNN)\n",
    "test_iter_CNN = chainer.iterators.SerialIterator(test, batchsize_CNN,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_CNN = training.StandardUpdater(train_iter_CNN, optimizer_CNN, converter=conv_distortion, device=gpu)\n",
    "trainer_CNN = training.Trainer(updater_CNN, (epoch_CNN, 'epoch'), out=\"result_CNN\")\n",
    "\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer_CNN.extend(extensions.Evaluator(test_iter_CNN, model_CNN, converter=conv_distortion, device=gpu), trigger=(1, \"epoch\"))\n",
    "\n",
    "# Reduce the learning rate by half every 25 epochs.\n",
    "trainer_CNN.extend(extensions.ExponentialShift('lr', 0.5),\n",
    "               trigger=(25, 'epoch'))\n",
    "\n",
    "# Take a snapshot at each epoch\n",
    "trainer_CNN.extend(extensions.snapshot(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer_CNN.extend(extensions.LogReport(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer_CNN.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']), trigger = (1, 'epoch'))\n",
    "\n",
    "trainer_CNN.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=50\n",
    "learnrate=0.05\n",
    "out=\"result_CNN\"\n",
    "epoch=200\n",
    "gpu=0\n",
    "original=23047\n",
    "percentage=15\n",
    "exponent=0.5\n",
    "sizes = [5, 100, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GradNet\n",
    "\n",
    "This can take a while. A pre-trained model can be found in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = get_cifar10()\n",
    "\n",
    "model_old = L.Classifier(net.cnn_cifar())\n",
    "\n",
    "optimizer_old = chainer.optimizers.SGD(learnrate)\n",
    "optimizer_old.setup(model_old)\n",
    "optimizer_old.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "    \n",
    "train_iter_old = chainer.iterators.SerialIterator (train [0:int (len (train)/2) ], 1, repeat=False, shuffle=False)\n",
    "test_iter_old = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_old = training.StandardUpdater(train_iter_old, optimizer_old, converter=conv_distortion, device=gpu)\n",
    "trainer_old = training.Trainer(updater_old, (1000, 'epoch'), out=out)\n",
    "\n",
    "chainer.serializers.load_npz ('./result_CNN/snapshot_iter_{}'.format(original), trainer_old, strict=False)\n",
    "\n",
    "if not os.path.exists('./result_gradnet/'):\n",
    "    os.mkdir('./result_gradnet/')\n",
    "\n",
    "if gpu >= 0:\n",
    "   # Make a specified GPU current\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model_old.to_gpu(gpu)  # Copy the model to the GPU\n",
    "   xp = cp\n",
    "else:\n",
    "   xp = np\n",
    "\n",
    "mins=xp.load(\"CNNgradmin_{}.npy\".format(original))\n",
    "maxes=xp.load(\"CNNgradmax_{}.npy\".format(original))\n",
    "ranges = maxes-mins\n",
    "\n",
    "linksizes = [link.W.size for link in list(model_old.predictor.links(skipself=True))]\n",
    "dividers = xp.cumsum(xp.array(linksizes))[:-1].tolist()\n",
    "\n",
    "model=net.gradnet(input_dividers = dividers, middle_sizes = sizes)\n",
    "\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "if gpu >=0 :\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model.to_gpu(gpu)\n",
    "\n",
    "train_iter_new = chainer.iterators.SerialIterator (train [int (len (train)/2)  : len (train)], 25, repeat=True, shuffle=True)\n",
    "gg = grad_gen_new (updater_old, train_iter_new, mins, ranges, test=False, xp=xp, realbatchsize=25, percentage=percentage, exponent=exponent)\n",
    "\n",
    "test_iter_new = chainer.iterators.SerialIterator (test, 25, repeat=False, shuffle=False)\n",
    "gg_test = grad_gen_new (updater_old, test_iter_new, mins, ranges, test=False, xp=xp, realbatchsize=25, percentage=percentage, exponent=exponent)\n",
    "\n",
    "\n",
    "t0=time.time()\n",
    "while train_iter_new.epoch < epoch:\n",
    "   grads_train, target_train = gg.__next__()\n",
    "   grads_train = chainer.Variable(grads_train)\n",
    "   target_train = chainer.Variable(target_train)\n",
    "   if gpu >=0:\n",
    "      grads_train.to_gpu(gpu)\n",
    "      target_train.to_gpu(gpu)\n",
    "\n",
    "   # Calculate the prediction of the network\n",
    "   prediction_train = model(grads_train)\n",
    "   # Calculate the loss with softmax_cross_entropy\n",
    "   loss = chainer.functions.softmax_cross_entropy(prediction_train, target_train)\n",
    "   # Calculate the gradients in the network\n",
    "   model.cleargrads()\n",
    "   loss.backward()\n",
    "   # Update all the trainable paremters\n",
    "   optimizer.update()\n",
    "   if optimizer.t%50==0:\n",
    "      print(optimizer.t)\n",
    "                      \n",
    "   if optimizer.t%100==0:\n",
    "      chainer.serializers.save_npz('result_gradnet/snapshot_iter_{}'.format(optimizer.t), model)\n",
    "      t1=time.time()\n",
    "      print(\"elapsed time:\", t1-t0)\n",
    "      mistakes = 0\n",
    "      for batch in gg_test:\n",
    "          grads_test, target_test = gg_test.__next__()\n",
    "          grads_test = chainer.Variable(grads_test)\n",
    "          target_test = chainer.Variable(target_test)\n",
    "          if gpu >=0:\n",
    "              grads_test.to_gpu(gpu)\n",
    "              target_test.to_gpu(gpu)\n",
    "          prediction_test = model(grads_test)\n",
    "          predicted_labels = cp.argmax(prediction_test.data, axis=1)\n",
    "          mistakes += cp.count_nonzero(predicted_labels-target_test.data)\n",
    "          accuracy = (len(test)-mistakes)/len(test)\n",
    "          print(\"accuracy =\", accuracy)\n",
    "      test_iter_new.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
