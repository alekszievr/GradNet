{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and testing of GradNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexievr/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    from chainer.cuda import cupy as cp\n",
    "except:\n",
    "    pass\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import TupleDataset\n",
    "from copy import deepcopy\n",
    "from chainer.datasets import get_cifar10\n",
    "from linalg import maxpercentile\n",
    "import time\n",
    "import net\n",
    "import augmentation\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_distortion(databatch, device, test=False):\n",
    "   \"\"\"Applies augmentation to the images and converts databatch into a tuple of form (images, labels) where both 'images' and 'labels' are arrays.\n",
    "      Moves data to gpu if device>=0.\"\"\"\n",
    "   if device>=0:\n",
    "        xp=cp\n",
    "   else:\n",
    "        xp=np\n",
    "   batchsize = len(databatch)\n",
    "   inputdata = [datatuple[0] for datatuple in databatch]\n",
    "   labels = xp.array([datatuple[1] for datatuple in databatch])\n",
    "   if device>=0:\n",
    "      inputdata = chainer.cuda.to_gpu(inputdata, device)\n",
    "      labels = chainer.cuda.to_gpu(labels, device)\n",
    "   distorted_input = augmentation.distortion_batch(inputdata, device, test=test)\n",
    "   return (distorted_input, labels) \n",
    "\n",
    "def grads (optimizer, data, label, mins=None, ranges=None, xp=np, percentage=15, exponent=0.5):\n",
    "   \"\"\"Creates gradient array for a given (data, label) pair. The neural net is contained by 'optimizer'.\n",
    "      'mins' and 'ranges' are arrays needed for scaling the gradients, 'exponent' is the exponent of the power norm.\n",
    "      'percentage' determines where we cut the gradient values to throw away the small ones.\"\"\"\n",
    "   opt = deepcopy (optimizer)\n",
    "   model = opt.target\n",
    "   net = opt.target.predictor\n",
    "   model.cleargrads ()\n",
    "   opt.update (model, data, label)\n",
    "   listoflinks = list(net.links(skipself=True))\n",
    "   gradients = xp.concatenate([maxpercentile(xp.ravel(link.W.grad), 15, xp) for link in listoflinks])\n",
    "   gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "   # scale norm + power norm:\n",
    "   normalized = xp.divide ((gradients-mins), xp.where (ranges!=0, ranges, 1))\n",
    "   n = xp.where (gradients!=0, normalized, 0)\n",
    "   out = xp.sign(n)*xp.power(xp.abs(n), exponent)\n",
    "   return out\n",
    "\n",
    "def grad_gen_new (updater, iterator, mins=None, ranges=None, test=False, xp=np, realbatchsize=25, percentage=15, exponent=0.5):\n",
    "   \"\"\"Generator that returns normalized gradient arrays given a dataset iterator and an updater. \n",
    "      The iterator gives data batches, the updater contains the neural net.\"\"\"\n",
    "   for (i, batch)  in enumerate (iterator):\n",
    "      batchsize=len (batch)\n",
    "      in_arr, truelabels = updater.converter (batch, updater.device)\n",
    "      random_labels = xp.random.randint (10, size=(batchsize,))\n",
    "      optimizer = updater._optimizers ['main']\n",
    "      gradients = xp.concatenate(([xp.expand_dims(grads(optimizer, xp.expand_dims(in_arr[j], 0), xp.expand_dims(random_labels[j], 0), mins, ranges, xp, percentage, exponent), axis=0) for j in range(batchsize)]))\n",
    "      gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "      yield gradients, truelabels\n",
    "\n",
    "\n",
    "def grad_gen_all (updater, iterator, mins=None, ranges=None,  test=False, xp=np, percentage=15):                                          \n",
    "      for (i, batch)  in enumerate (iterator):                                                                                                                                                \n",
    "         batchsize=len (batch)                                                                                                                                                                \n",
    "         in_arr, truelabels = updater.converter (batch, updater.device, test=test)                                                                                                            \n",
    "         optimizer = updater._optimizers ['main']                                                                                                                                             \n",
    "         gradients = xp.concatenate(([xp.expand_dims(grads(optimizer, xp.expand_dims(in_arr[j], 0), xp.array ([k]), mins, ranges, xp, percentage), axis=0) for j\\\n",
    " in range(batchsize) for k in range (10)]))                                                                                                                                                   \n",
    "         gradients = xp.ndarray.astype (gradients, xp.float32)                                                                                                                                \n",
    "         yield gradients, truelabels                                                                                                                                                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Cifar-10 dataset. \n",
    "\n",
    "We split the training data into two equally sized sets. We use the first one to train the original CNN network and the second one to train the GradNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset:\n",
    "train, test = get_cifar10()\n",
    "train_CNN = train [0:int (len (train)/2) ]\n",
    "train_gradnet = train [int (len (train)/2)  : len (train)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our pre-trained GradNet using Cifar-10 or other datasets\n",
    "\n",
    "In this following section, we measure the performance of our pre-trained GradNet on the Cifar-10 dataset. \n",
    "\n",
    "To use your own image dataset, convert your images into numpy or cupy array 'images' of shape (number_of_images, 3, 32, 32) and your labels into numpy or cupy array 'labels' of shape (number_of_images, ). Then, create a TupleDataset:\n",
    "\n",
    "test = TupleDataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000 images done\n",
      "500/10000 images done\n",
      "1000/10000 images done\n",
      "1500/10000 images done\n",
      "2000/10000 images done\n",
      "2500/10000 images done\n",
      "3000/10000 images done\n",
      "3500/10000 images done\n",
      "4000/10000 images done\n",
      "4500/10000 images done\n",
      "5000/10000 images done\n",
      "5500/10000 images done\n",
      "6000/10000 images done\n",
      "6500/10000 images done\n",
      "7000/10000 images done\n"
     ]
    }
   ],
   "source": [
    "gpu=0 #set this to -1 if GPU is not available\n",
    "model_CNN = L.Classifier (net.cnn_cifar ())\n",
    "if gpu >= 0:\n",
    "    chainer.cuda.get_device_from_id(gpu).use()\n",
    "    model_CNN.to_gpu()  # Copy the model to the GPU\n",
    "    xp=cp\n",
    "else:\n",
    "    xp=np\n",
    "\n",
    "optimizer_CNN = chainer.optimizers.SGD(0.05)\n",
    "optimizer_CNN.setup(model_CNN)\n",
    "optimizer_CNN.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "train_iter_CNN = chainer.iterators.SerialIterator(train_CNN, 64)\n",
    "test_iter_CNN = chainer.iterators.SerialIterator(test, 64,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_CNN = training.StandardUpdater(train_iter_CNN, optimizer_CNN, converter=conv_distortion, device=gpu)\n",
    "trainer_CNN = training.Trainer(updater_CNN, (100, 'epoch'), out=\"result_CNN\")\n",
    "\n",
    "chainer.serializers.load_npz ('./result_CNN/snapshot_iter_91016', trainer_CNN, strict=False)\n",
    "\n",
    "mins=xp.load(\"CNNgradmin_91016.npy\")\n",
    "maxes=xp.load(\"CNNgradmax_91016.npy\")\n",
    "ranges = maxes-mins\n",
    "\n",
    "linksizes = [link.W.size for link in list(model_CNN.predictor.links(skipself=True))]\n",
    "dividers = xp.cumsum(xp.array(linksizes))[:-1].tolist()\n",
    "\n",
    "model=net.gradnet(input_dividers = dividers, middle_sizes = [5, 100, 25])\n",
    "\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "if gpu >=0 :\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model.to_gpu(gpu)\n",
    "\n",
    "chainer.serializers.load_npz ('./result_gradnet/snapshot_iter_200000', model, strict=False)\n",
    "\n",
    "test_iter_new = chainer.iterators.SerialIterator (test, 1, repeat=False, shuffle=False)\n",
    "gg_test = grad_gen_all (updater_CNN, test_iter_new, mins=mins, ranges = ranges, test=True, xp=xp) \n",
    "\n",
    "correct = 0                                                                                                                                                                                 \n",
    "for (i, databatch) in enumerate (gg_test):                                                                                                                                                  \n",
    "   gradients, truelabel = databatch                                                                                                                                                         \n",
    "   guessed_label = xp.argmax (xp.sum(model (gradients).data, axis=0))                                                                                                                  \n",
    "   if guessed_label==truelabel:                                                                                                                                                             \n",
    "      correct+=1\n",
    "   if i%500 ==0:\n",
    "      print(\"{}/10000\".format(i), \"images done\")\n",
    "print(correct/len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the original CNN network.\n",
    "\n",
    "This can take a while. A pre-trained model can be found in the repository. To skip this training, jump to the \"training the GradNet\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize_CNN=64\n",
    "learnrate_CNN=0.05\n",
    "epoch_CNN=100\n",
    "gpu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J1           1.48478     1.63433               0.421875       0.403165                  232.612       \n",
      "\u001b[J2           1.27392     1.48166               0.609375       0.46865                   462.268       \n",
      "\u001b[J3           1.25437     1.38152               0.546875       0.503583                  679.968       \n",
      "\u001b[J4           1.42943     1.29438               0.515625       0.542596                  903.339       \n",
      "\u001b[J5           1.19399     1.26791               0.59375        0.554439                  1124.06       \n",
      "\u001b[J6           1.24174     1.24069               0.59375        0.562898                  1357.5        \n",
      "\u001b[J7           1.14403     1.22494               0.53125        0.583002                  1574.54       \n",
      "\u001b[J8           1.00215     1.1674                0.671875       0.592357                  1812.38       \n",
      "\u001b[J9           1.02669     1.18634               0.625          0.584096                  2039.92       \n",
      "\u001b[J10          0.994309    1.13364               0.65625        0.600916                  2276.79       \n",
      "\u001b[J11          1.08708     1.13362               0.640625       0.607285                  2507.51       \n",
      "\u001b[J12          0.952616    1.16692               0.65625        0.605494                  2733.88       \n",
      "\u001b[J13          1.16696     1.12665               0.578125       0.618531                  2952.08       \n",
      "\u001b[J14          0.913735    1.05172               0.6875         0.640028                  3170.92       \n",
      "\u001b[J15          1.12582     1.04442               0.5625         0.634952                  3380.72       \n",
      "\u001b[J16          1.01378     1.01031               0.6875         0.650975                  3595.79       \n",
      "\u001b[J17          0.778743    1.01667               0.796875       0.649283                  3808.05       \n",
      "\u001b[J18          0.808866    1.048                 0.734375       0.632365                  4028.15       \n",
      "\u001b[J19          0.94933     0.982778              0.609375       0.658141                  4243.48       \n",
      "\u001b[J20          0.98161     1.00847               0.671875       0.652468                  4460.57       \n",
      "\u001b[J21          0.765105    0.990588              0.71875        0.653264                  4675.52       \n",
      "\u001b[J22          0.724871    0.97363               0.71875        0.671079                  4908.11       \n",
      "\u001b[J23          0.887182    0.991499              0.65625        0.664411                  5145.57       \n",
      "\u001b[J24          0.921455    0.971232              0.703125       0.670084                  5360.58       \n",
      "\u001b[J25          0.951832    0.97567               0.671875       0.661425                  5573.11       \n",
      "\u001b[J26          0.625606    0.909848              0.765625       0.688097                  5791.44       \n",
      "\u001b[J27          0.780207    0.899067              0.75           0.686704                  6017.49       \n",
      "\u001b[J28          0.981831    0.914212              0.671875       0.690983                  6244.79       \n",
      "\u001b[J29          0.982955    0.889776              0.75           0.694367                  6462.6        \n",
      "\u001b[J30          0.827575    0.897916              0.703125       0.695362                  6666.81       \n",
      "\u001b[J31          0.583432    0.873258              0.78125        0.700736                  6876.97       \n",
      "\u001b[J32          0.817353    0.86648               0.71875        0.709494                  7086.21       \n",
      "\u001b[J33          0.950619    0.886616              0.703125       0.69576                   7303.34       \n",
      "\u001b[J34          0.804541    0.876534              0.734375       0.701035                  7520.25       \n",
      "\u001b[J35          0.72175     0.886815              0.765625       0.696756                  7735.46       \n",
      "\u001b[J36          0.859894    0.930495              0.703125       0.688893                  7954.58       \n",
      "\u001b[J37          0.753644    0.888723              0.703125       0.707604                  8175.03       \n",
      "\u001b[J38          0.803938    0.855302              0.703125       0.705713                  8438.69       \n",
      "\u001b[J39          0.519151    0.921695              0.859375       0.68342                   8663.86       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e46f1cefa694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m      'main/accuracy', 'validation/main/accuracy', 'elapsed_time']), trigger = (1, 'epoch'))\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrainer_CNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0min_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-798ad467d069>\u001b[0m in \u001b[0;36mconv_distortion\u001b[0;34m(databatch, device, test)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0minputdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m    \u001b[0mdistorted_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistortion_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdistorted_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/idms/home/alexievr/GradNet/augmentation.py\u001b[0m in \u001b[0;36mdistortion_batch\u001b[0;34m(images, device, test)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistortion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/idms/home/alexievr/GradNet/augmentation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistortion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/idms/home/alexievr/GradNet/augmentation.py\u001b[0m in \u001b[0;36mdistortion\u001b[0;34m(image, test, xp)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m'''alkalmazza az osszes torzitast'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_flip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;31m# image = random_brightness(image, test=test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/idms/home/alexievr/GradNet/augmentation.py\u001b[0m in \u001b[0;36mrandom_flip\u001b[0;34m(image, test, xp)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/cupy/random/sample.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(low, high, size, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \"\"\"\n\u001b[1;32m    111\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/cupy/random/generator.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, low, high, size, dtype)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 'range is currently not supported')\n\u001b[1;32m    789\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training of the original CNN network:\n",
    "model_CNN = L.Classifier (net.cnn_cifar ())\n",
    "if gpu >= 0:\n",
    "    # Make a specified GPU current\n",
    "    chainer.cuda.get_device_from_id(gpu).use()\n",
    "    model_CNN.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "optimizer_CNN = chainer.optimizers.SGD(learnrate_CNN)\n",
    "optimizer_CNN.setup(model_CNN)\n",
    "optimizer_CNN.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "\n",
    "train_iter_CNN = chainer.iterators.SerialIterator(train_CNN, batchsize_CNN)\n",
    "test_iter_CNN = chainer.iterators.SerialIterator(test, batchsize_CNN,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_CNN = training.StandardUpdater(train_iter_CNN, optimizer_CNN, converter=conv_distortion, device=gpu)\n",
    "trainer_CNN = training.Trainer(updater_CNN, (epoch_CNN, 'epoch'), out=\"result_CNN\")\n",
    "\n",
    "\n",
    "# Evaluate the model with the test dataset for each epoch\n",
    "trainer_CNN.extend(extensions.Evaluator(test_iter_CNN, model_CNN, converter=conv_distortion, device=gpu), trigger=(1, \"epoch\"))\n",
    "\n",
    "# Reduce the learning rate by half every 25 epochs.\n",
    "trainer_CNN.extend(extensions.ExponentialShift('lr', 0.5),\n",
    "               trigger=(25, 'epoch'))\n",
    "\n",
    "# Take a snapshot at each epoch\n",
    "trainer_CNN.extend(extensions.snapshot(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Write a log of evaluation statistics for each epoch\n",
    "trainer_CNN.extend(extensions.LogReport(), trigger=(1, 'epoch'))\n",
    "\n",
    "# Print selected entries of the log to stdout\n",
    "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
    "# \"validation\" refers to the default name of the Evaluator extension.\n",
    "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
    "# either the updater or the evaluator.\n",
    "trainer_CNN.extend(extensions.PrintReport(\n",
    "    ['epoch', 'main/loss', 'validation/main/loss',\n",
    "     'main/accuracy', 'validation/main/accuracy', 'elapsed_time']), trigger = (1, 'epoch'))\n",
    "\n",
    "trainer_CNN.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the GradNet\n",
    "\n",
    "This can take a while. A pre-trained model can be found in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=50\n",
    "learnrate=0.05\n",
    "out=\"result_CNN\"\n",
    "epoch=200\n",
    "gpu=0\n",
    "original=91016\n",
    "percentage=15\n",
    "exponent=0.5\n",
    "sizes = [5, 100, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "elapsed time: 549.2684609889984\n",
      "accuracy = 0.7716\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "elapsed time: 2741.32288312912\n",
      "accuracy = 0.7822\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "elapsed time: 4951.305458784103\n",
      "accuracy = 0.7847\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "elapsed time: 7073.333925962448\n",
      "accuracy = 0.7855\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "elapsed time: 9261.793446063995\n",
      "accuracy = 0.7881\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "elapsed time: 11539.685985088348\n"
     ]
    }
   ],
   "source": [
    "model_old = L.Classifier(net.cnn_cifar())\n",
    "\n",
    "optimizer_old = chainer.optimizers.SGD(learnrate)\n",
    "optimizer_old.setup(model_old)\n",
    "optimizer_old.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "    \n",
    "train_iter_old = chainer.iterators.SerialIterator (train_CNN, 1, repeat=False, shuffle=False)\n",
    "test_iter_old = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_old = training.StandardUpdater(train_iter_old, optimizer_old, converter=conv_distortion, device=gpu)\n",
    "trainer_old = training.Trainer(updater_old, (1000, 'epoch'), out=out)\n",
    "\n",
    "chainer.serializers.load_npz ('./result_CNN/snapshot_iter_{}'.format(original), trainer_old, strict=False)\n",
    "\n",
    "if not os.path.exists('./result_gradnet/'):\n",
    "    os.mkdir('./result_gradnet/')\n",
    "\n",
    "if gpu >= 0:\n",
    "   # Make a specified GPU current\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model_old.to_gpu(gpu)  # Copy the model to the GPU\n",
    "   xp = cp\n",
    "else:\n",
    "   xp = np\n",
    "\n",
    "mins=xp.load(\"CNNgradmin_{}.npy\".format(original))\n",
    "maxes=xp.load(\"CNNgradmax_{}.npy\".format(original))\n",
    "ranges = maxes-mins\n",
    "\n",
    "linksizes = [link.W.size for link in list(model_old.predictor.links(skipself=True))]\n",
    "dividers = xp.cumsum(xp.array(linksizes))[:-1].tolist()\n",
    "\n",
    "model=net.gradnet(input_dividers = dividers, middle_sizes = sizes)\n",
    "\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "if gpu >=0 :\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model.to_gpu(gpu)\n",
    "\n",
    "train_iter_new = chainer.iterators.SerialIterator (train [int (len (train)/2)  : len (train)], 25, repeat=True, shuffle=True)\n",
    "gg = grad_gen_new (updater_old, train_iter_new, mins, ranges, test=False, xp=xp, realbatchsize=25, percentage=percentage, exponent=exponent)\n",
    "\n",
    "test_iter_new = chainer.iterators.SerialIterator (test, 1, repeat=False, shuffle=False)\n",
    "gg_test = grad_gen_all (updater_old, test_iter_new, mins=mins, ranges = ranges, test=True, xp=xp) \n",
    "\n",
    "t0=time.time()\n",
    "while train_iter_new.epoch < epoch:\n",
    "   grads_train, target_train = gg.__next__()\n",
    "   grads_train = chainer.Variable(grads_train)\n",
    "   target_train = chainer.Variable(target_train)\n",
    "   if gpu >=0:\n",
    "      grads_train.to_gpu(gpu)\n",
    "      target_train.to_gpu(gpu)\n",
    "\n",
    "   # Calculate the prediction of the network\n",
    "   prediction_train = model(grads_train)\n",
    "   # Calculate the loss with softmax_cross_entropy\n",
    "   loss = chainer.functions.softmax_cross_entropy(prediction_train, target_train)\n",
    "   # Calculate the gradients in the network\n",
    "   model.cleargrads()\n",
    "   loss.backward()\n",
    "   # Update all the trainable paremters\n",
    "   optimizer.update()\n",
    "   if optimizer.t%50==0:\n",
    "      print(optimizer.t)\n",
    "                      \n",
    "   if optimizer.t%1000==0:\n",
    "      chainer.serializers.save_npz('result_gradnet/snapshot_iter_{}'.format(optimizer.t), model)\n",
    "      t1=time.time()\n",
    "      print(\"elapsed time:\", t1-t0)\n",
    "      correct = 0\n",
    "      for (i, databatch) in enumerate (gg_test):                                                                                                                                                  \n",
    "          gradients, truelabel = databatch                                                                                                                                                         \n",
    "          guessed_label = xp.argmax (xp.sum(model (gradients).data, axis=0))                                                                                                                  \n",
    "          if guessed_label==truelabel:                                                                                                                                                             \n",
    "              correct+=1\n",
    "      print(\"accuracy =\", correct/len(test)) \n",
    "      test_iter_new.reset()\n",
    "      gg_test = grad_gen_all (updater_old, test_iter_new, mins=mins, ranges = ranges, test=True, xp=xp) \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
