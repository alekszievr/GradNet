{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from chainer.cuda import cupy as cp\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "from copy import deepcopy\n",
    "from chainer.datasets import get_cifar10\n",
    "from linalg import maxpercentile\n",
    "import time\n",
    "import net\n",
    "import augmentation\n",
    "import pdb\n",
    "\n",
    "def conv_distortion(databatch, device, test=False):\n",
    "   \"\"\"converter a cifar datasethez: elvegzi az augmentaciot es meghagyja a cimkeket\"\"\"\n",
    "   batchsize = len(databatch)\n",
    "   inputdata = [datatuple[0] for datatuple in databatch]\n",
    "   labels = np.array([datatuple[1] for datatuple in databatch])\n",
    "   if device>=0:\n",
    "      inputdata = chainer.cuda.to_gpu(inputdata, device)\n",
    "      labels = chainer.cuda.to_gpu(labels, device)\n",
    "   distorted_input = augmentation.distortion_batch(inputdata, device, test=test)\n",
    "   return (distorted_input, labels)   \n",
    "\n",
    "def grads (optimizer, data, label, mins=None, ranges=None, xp=np, percentage=15, exponent=0.5):\n",
    "   opt = deepcopy (optimizer)\n",
    "   model = opt.target\n",
    "   net = opt.target.predictor\n",
    "   model.cleargrads ()\n",
    "   opt.update (model, data, label)\n",
    "   listoflinks = list(net.links(skipself=True))\n",
    "   gradients = xp.concatenate([maxpercentile(xp.ravel(link.W.grad), 15, xp) for link in listoflinks])\n",
    "   gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "   # scale norm + power norm:\n",
    "   normalized = xp.divide ((gradients-mins), xp.where (ranges!=0, ranges, 1))\n",
    "   n = xp.where (gradients!=0, normalized, 0)\n",
    "   out = xp.sign(n)*xp.power(xp.abs(n), exponent)\n",
    "   return out\n",
    "\n",
    "def grad_gen_new (updater, iterator, mins=None, ranges=None, test=False, xp=np, realbatchsize=25, percentage=15, exponent=0.125):\n",
    "   for (i, batch)  in enumerate (iterator):\n",
    "      batchsize=len (batch)\n",
    "      in_arr, truelabels = updater.converter (batch, updater.device)\n",
    "      random_labels = xp.random.randint (10, size=(batchsize,))\n",
    "      optimizer = updater._optimizers ['main']\n",
    "      gradients = xp.concatenate(([xp.expand_dims(grads(optimizer, xp.expand_dims(in_arr[j], 0), xp.expand_dims(random_labels[j], 0), mins, ranges, xp, percentage, exponent), axis=0) for j in range(batchsize)]))\n",
    "      gradients = xp.ndarray.astype (gradients, xp.float32)\n",
    "      for k in range (int (batchsize/realbatchsize)):\n",
    "         g = gradients [k*realbatchsize:(k+1)* realbatchsize, :]\n",
    "         o = truelabels [k*realbatchsize:(k+1)*realbatchsize]\n",
    "         yield g, o\n",
    "\n",
    "batchsize=50\n",
    "learnrate=0.05\n",
    "out=\"result\"\n",
    "epoch=200\n",
    "gpu=0\n",
    "original=23047\n",
    "normalization=\"std\"\n",
    "percentage=15\n",
    "exponent=0.5\n",
    "sizes = [5, 100, 25]\n",
    "\n",
    "train, test = get_cifar10()\n",
    "\n",
    "model_old = L.Classifier(net.cnn_cifar())\n",
    "\n",
    "optimizer_old = chainer.optimizers.SGD(learnrate)\n",
    "optimizer_old.setup(model_old)\n",
    "optimizer_old.add_hook(chainer.optimizer.WeightDecay(5e-4))\n",
    "    \n",
    "train_iter_old = chainer.iterators.SerialIterator (train [0:int (len (train)/2) ], 1, repeat=False, shuffle=False)\n",
    "test_iter_old = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                repeat=False, shuffle=False)\n",
    "# Set up a trainer\n",
    "updater_old = training.StandardUpdater(train_iter_old, optimizer_old, converter=conv_distortion, device=gpu)\n",
    "trainer_old = training.Trainer(updater_old, (1000, 'epoch'), out=out)\n",
    "\n",
    "chainer.serializers.load_npz ('./result_original/snapshot_iter_{}'.format(original), trainer_old, strict=False)\n",
    "\n",
    "if gpu >= 0:\n",
    "   # Make a specified GPU current\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model_old.to_gpu(gpu)  # Copy the model to the GPU\n",
    "   xp = cp\n",
    "else:\n",
    "   xp = np\n",
    "\n",
    "mins=xp.load(\"CNNgradmin_{}.npy\".format(original))\n",
    "maxes=xp.load(\"CNNgradmax_{}.npy\".format(original))\n",
    "ranges = maxes-mins\n",
    "\n",
    "linksizes = [link.W.size for link in list(model_old.predictor.links(skipself=True))]\n",
    "dividers = xp.cumsum(xp.array(linksizes))[:-1].tolist()\n",
    "\n",
    "model=net.gradnet(input_dividers = dividers, middle_sizes = sizes)\n",
    "\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "\n",
    "if gpu >=0 :\n",
    "   chainer.cuda.get_device_from_id(gpu).use()\n",
    "   model.to_gpu(gpu)\n",
    "\n",
    "train_iter_new = chainer.iterators.SerialIterator (train [int (len (train)/2)  : len (train)], 25, repeat=True, shuffle=True)\n",
    "gg = grad_gen_new (updater_old, train_iter_new, mins, ranges, test=False, xp=xp, realbatchsize=25, percentage=percentage, exponent=exponent)\n",
    "\n",
    "t0=time.time()\n",
    "while train_iter_new.epoch < epoch:\n",
    "   grads_train, target_train = gg.__next__()\n",
    "   grads_train = chainer.Variable(grads_train)\n",
    "   target_train = chainer.Variable(target_train)\n",
    "   if gpu >=0:\n",
    "      grads_train.to_gpu(gpu)\n",
    "      target_train.to_gpu(gpu)\n",
    "\n",
    "   # Calculate the prediction of the network\n",
    "   prediction_train = model(grads_train)\n",
    "   # Calculate the loss with softmax_cross_entropy\n",
    "   loss = chainer.functions.softmax_cross_entropy(prediction_train, target_train)\n",
    "   # Calculate the gradients in the network\n",
    "   model.cleargrads()\n",
    "   loss.backward()\n",
    "   # Update all the trainable paremters\n",
    "   optimizer.update()\n",
    "   if optimizer.t%50==0:\n",
    "      print(optimizer.t)\n",
    "                      \n",
    "   if optimizer.t%500==0:\n",
    "      print (optimizer.t)\n",
    "      chainer.serializers.save_npz('result_gradnet/snapshot_iter_{}'.format(optimizer.t), model)\n",
    "      t1=time.time()\n",
    "      print(\"time:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
